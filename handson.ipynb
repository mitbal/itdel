{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning in Computer Vision Hands-on using scikit-learn library for pemilu digits recognition\n",
    "\n",
    "### Background\n",
    "In 2014 Presidential election, the result for each election booth in the form of C1 is uploaded into KPU webserver.\n",
    "A crowdsourcing effort, kawalpemilu.org, label the tally result into number that can be counted by computer. Utilizing and combining these two sources of data, we can then scrap the image and the label to devise image digit dataset. For more information regarding the digit extraction procedure, you can visit https://mitbal.wordpress.com/2014/10/10/pemilu-presiden-indonesia-2014-pendekatan-pembelajaran-mesin/\n",
    "\n",
    "Computer vision is a field that deals with extracting information and understanding from digital images as its input. For examples, what kind of object exist in this image (object recognition), where is it located (object localization), and so on. It usually comprises of both image processing and machine learning techniques. In this project, we will try to automatically predict each individual digit in the image using machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About this setup\n",
    "Python is one of the most popular chosen language for data science and machine learning. One of the reasons is it is clear and concise language, easy to use and easy to code. One of the downside is maybe its slow execution time. However, for critical part of the code is usually implemented in C, and then called via wrapper in python. Therefore you get the expressive power in python with speed.\n",
    "\n",
    "In this project, we use one of the most popular machine learning, scikit-learn. It provides complete end-to-end machine learning pipeline, starting from preprocessing to model selection. It is also regularly updated and even provides new and state-of-the-art algorithm.\n",
    "\n",
    "Jupyter notebook is a platform to code, document, and visualize experiment in machine learning in place. Think Microsoft Word for data science, although it is hard to use in collaborative manner (unlike Google docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just import all necessary modules for this project. Library in python can easily be installed via package manager pip or conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data\n",
    "Now let's read all image and load it into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir = '/home/mitbal/workspace/itdel/pemilu/'\n",
    "dataset = np.zeros((20000, 50*100))\n",
    "label = np.zeros((20000, ))\n",
    "counter = 0\n",
    "for i in xrange(10):\n",
    "    files = glob.glob(input_dir + str(i) + '/*.png')\n",
    "    \n",
    "    for f in files:\n",
    "        im = io.imread(f)\n",
    "        dataset[counter, :] = im.flatten()\n",
    "        label[counter] = i\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total there are 20000 images, for class 0 to 9. Each class have 2000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration\n",
    "We'll sample first 100 images from each classes and plot the sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = np.zeros((1000, 5000))\n",
    "for i in xrange(10):\n",
    "    for j in xrange(10):\n",
    "        for k in xrange(10):\n",
    "            img[k*100:(k+1)*100, i*500+j*50:i*500+(j+1)*50] = np.reshape(dataset[i*2000+j*10+k, :], (100, 50))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to just look into the data before we even applying any machine learning algorithm. Just to get the feel of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll investigate how the data live in high-dimensional feature space using 2 popular visualization techniques, PCA and t-SNE.\n",
    "\n",
    "PCA is using linear transformation to the direction of the highest variance.\n",
    "Meanwhile, t-sne can detect non-linear manifold in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_pca(X, y):\n",
    "    pca = PCA(n_components=2)\n",
    "    output = pca.fit_transform(X)\n",
    "\n",
    "    plt.subplots(figsize=(30, 20))\n",
    "    categories = set(y)\n",
    "    for cat in categories:\n",
    "        subset = np.array([a for a,b in zip(output, y) if b == cat])\n",
    "        plt.plot(subset[:, 0], subset[:, 1], color=np.random.random((3,)), label=str(cat), marker='o', linewidth=0)\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_tsne(X, y):\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "    output = tsne.fit_transform(X)\n",
    "\n",
    "    plt.subplots(figsize=(30, 20))\n",
    "    categories = set(y)\n",
    "    for cat in categories:\n",
    "        subset = np.array([a for a,b in zip(output, y) if b == cat])\n",
    "        plt.plot(subset[:, 0], subset[:, 1], color=np.random.random((3,)), label=str(cat), marker='o', linewidth=0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pca(dataset, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_tsne(dataset[range(0, 20000, 4), :], label[range(0, 20000, 4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in above figure, the class is overlapped. The PCA plot is even harder to see. This suggest that it might be difficult to classify them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Divide data into training and testing\n",
    "\n",
    "We divide our dataset into 2 parts, training and testing.\n",
    "Reducing training error into zero is pretty trivial using algorithm such as nearest neighbor. However, that's not our goal. Our goal is to reduce generalization error, the error the algorithm make when it face unseen data.\n",
    "That's why we train using training dataset, and then perform evaluation using testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "Some algorithm is sensitive to the scale of the data. That's why it is good practice to normalize the data first before inputted into machine learning algorithm.\n",
    "\n",
    "There are several different techniques for normalization. One of the most commonly used is to normalize each features by reducing the mean and divide it using its standard deviation (usually called standardization) and then normalized the length of each input into unit vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our first parameterized model, logistic regression. Or more aptly, softmax regression because it is multi class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='log', alpha=0.001)\n",
    "sgd.fit(X_train, y_train)\n",
    "prediction = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_acc(prediction, label):\n",
    "    print 'acc', (sum(prediction == label) / float(len(label))) * 100, '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_acc(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good way to inspect the performance is by looking at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix(prediction, label):\n",
    "    matrix = np.zeros((10, 10))\n",
    "    \n",
    "    for i, j in zip(prediction, label):\n",
    "        matrix[int(i), int(j)] += 1\n",
    "    \n",
    "    print matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_confusion_matrix(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try cross validation.\n",
    "to find the best hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = SGDClassifier(loss='log', alpha=0.0001)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "alphas = [0.00001*math.pow(10, i) for i in xrange(4)]\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    clf = SGDClassifier(loss='log', alpha=alpha)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    print alpha, scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like it perform best with alpha equals to Lets try on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='log', alpha=0.0001)\n",
    "sgd.fit(X_train, y_train)\n",
    "prediction = sgd.predict(X_test)\n",
    "\n",
    "get_acc(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature engineering\n",
    "Okay, now let's try different features description. In practice, better representation is often\n",
    "\n",
    "#### PCA\n",
    "Before, we use PCA for visualization. However, PCA is often used as dimensionality reduction techniques.\n",
    "It used to be popular feature for face detection. In this case it is called as eigenface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=300)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_train = pca.transform(X_train)\n",
    "reduced_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='log')\n",
    "sgd.fit(reduced_train, y_train)\n",
    "prediction = sgd.predict(reduced_test)\n",
    "\n",
    "get_acc(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we get almost the same result from using raw pixel features. However, now we can achieve this using much lower number of features (300 vs 5000). In practice, using fewer features is favoured as it will give usually better generalization due to bias-variance theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOG features\n",
    "Histogram of Gradient (HOG) is a popular, originating in used for pedestrian detection.\n",
    "\n",
    "The basic idea is to count the gradient/direction in each cells of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "example_image = np.reshape(dataset[2001, :], (100, 50))\n",
    "fd, hog_image = hog(example_image, orientations=8, pixels_per_cell=(12, 12), cells_per_block=(1, 1), visualise=True)\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train = len(X_train)\n",
    "num_features = fd.shape[0]\n",
    "fd_trains = np.zeros((num_train, num_features))\n",
    "for i in xrange(num_train):\n",
    "    fd, hog_image = hog(np.reshape(X_train[i, :], (100, 50)), orientations=8, pixels_per_cell=(12, 12), cells_per_block=(1, 1), visualise=True)\n",
    "    fd_trains[i, :] = fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_test = len(X_test)\n",
    "fd_test = np.zeros((num_test, num_features))\n",
    "for i in xrange(num_test):\n",
    "    fd, hog_image = hog(np.reshape(X_test[i, :], (100, 50)), orientations=8, pixels_per_cell=(12, 12), cells_per_block=(1, 1), visualise=True)\n",
    "    fd_test[i, :] = fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='log')\n",
    "sgd.fit(fd_trains, y_train)\n",
    "prediction = sgd.predict(fd_test)\n",
    "\n",
    "get_acc(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our better number yet. Better features usually have better separability. Let's' plot another t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_tsne(fd_trains[range(0, num_train, 4), :], y_train[range(0, num_train, 4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to using to increase the accuracy, without cheating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "So there you have it.\n",
    "\n",
    "Machine learning algorithm is just one piece in the pipeline. \n",
    "Sometimes it is even better to have clean data (both for its label and its raw value) and better designed features/representatives. Algorithm then come last. The hierarchy is usually like this.\n",
    "\n",
    "DATA > FEATURE > ALGORITHM\n",
    "\n",
    "For successful machine learning project it is also necessary to have these characteristics:\n",
    "- Proper data with correct label\n",
    "- Proper preprocessing\n",
    "- Proper evaluation\n",
    "\n",
    "If we have deployed the system, then it will then need proper monitoring, proper tracking, and proper logging. Especially for ever changing problem such as for fraud detection system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
